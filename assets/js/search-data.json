{
  
    
        "post0": {
            "title": "Part 2 - Featurising HN items for HackerMark",
            "content": "Load saved HN items data . import pickle with open(&#39;data/hn.stories.dt.pickle&#39;, &#39;rb&#39;) as inpf: df_stories = pickle.load(inpf) print(df_stories.shape) display(df_stories.head()) . (3683879, 3) . title id timestamp . 0 Is $100,000 middle class in America? | 15559979 | 2017-10-26 15:49:32+00:00 | . 1 The End of Xeon Phi – It’s Xeon and Maybe GPUs... | 17637302 | 2018-07-29 09:54:26+00:00 | . 2 Apple Store App knows when you walk into an Ap... | 3139391 | 2011-10-21 12:10:19+00:00 | . 3 The Difference in Work for Beginner and Profes... | 19918406 | 2019-05-15 11:42:08+00:00 | . 4 Jump2header: Add markdown links to the top of ... | 22789110 | 2020-04-05 22:02:34+00:00 | . Pre-processing the stories . import re def clean_text(text): # Drop quotes, commas and dots text = re.sub(&#39;[ &#39; &quot; .,]&#39;, &#39;&#39;, text) # Replace all symbols (except: $, ?, !) with whitespace text = re.sub(&#39;[^a-zA-Z0-9$ ? !]&#39;, &#39; &#39;, text) # Lower case text = text.lower() # Replace digits with &lt;NUM&gt; text = re.sub(&#39; d+&#39;, &#39; &lt;NUM&gt; &#39;, text) # Pad title with &lt;START&gt; and &lt;END&gt; tokens text = &#39;&lt;START&gt; &#39; + text + &#39; &lt;END&gt;&#39; return text # Drop stories that are without titles df_stories = df_stories[~df_stories[&#39;title&#39;].isna()] # Apply the pre-processing function df_stories[&#39;title-proc&#39;] = df_stories[&#39;title&#39;].apply(clean_text) print(df_stories.shape) display(df_stories.head()) . (3500863, 4) . title id timestamp title-proc . 0 Is $100,000 middle class in America? | 15559979 | 2017-10-26 15:49:32+00:00 | &lt;START&gt; is $ &lt;NUM&gt; middle class in america? &lt;... | . 1 The End of Xeon Phi – It’s Xeon and Maybe GPUs... | 17637302 | 2018-07-29 09:54:26+00:00 | &lt;START&gt; the end of xeon phi it s xeon and ma... | . 2 Apple Store App knows when you walk into an Ap... | 3139391 | 2011-10-21 12:10:19+00:00 | &lt;START&gt; apple store app knows when you walk in... | . 3 The Difference in Work for Beginner and Profes... | 19918406 | 2019-05-15 11:42:08+00:00 | &lt;START&gt; the difference in work for beginner an... | . 4 Jump2header: Add markdown links to the top of ... | 22789110 | 2020-04-05 22:02:34+00:00 | &lt;START&gt; jump &lt;NUM&gt; header add markdown links ... | . Prepare data for Gensim&#39;s word2vec module . from nltk.tokenize import WhitespaceTokenizer tokenizer = WhitespaceTokenizer() n = 10 # For dev # n = df_stories.shape[0] # Actual training docs_tokenized = [ tokenizer.tokenize(row[&#39;title-proc&#39;]) for i, row in df_stories[:n].iterrows() ] print(&#39;Tokenized {} docs&#39;.format(len(docs_tokenized))) . Tokenized 10 docs . Checkpoint - tokenised data, ready for Gensim&#39;s word2vec . # Save with open(&#39;data/docs.pickle&#39;, &#39;wb&#39;) as outpf: pickle.dump(docs_tokenized, outpf) # Load with open(&#39;data/docs.pickle&#39;, &#39;rb&#39;) as inpf: docs_tokenized = pickle.load(inpf) . Saving data at intermediate steps is a good practice when developing ML models. Pick-up from where you left and don&#39;t wait the boring wait. Leaves you more time for building out the rest of the pipeline. . Train the word2vec model . from gensim.models.word2vec import Word2Vec from gensim.models.callbacks import CallbackAny2Vec # Hyperparameters EMB_DIM = 64 ITERS_COUNT = 5 # Dev value; I had used &#39;50&#39; for actual training WINDOW_LEN = 4 WORD_COUNT_MIN = 1 # A utility class to print loss after every epoch class callback(CallbackAny2Vec): &#39;&#39;&#39;Callback to print loss after each epoch.&#39;&#39;&#39; def __init__(self): self.epoch = 0 def on_epoch_end(self, model): loss = model.get_latest_training_loss() print(&#39;Loss after epoch {}: {}&#39;.format(self.epoch, loss)) self.epoch += 1 model = Word2Vec( docs_tokenized, iter=ITERS_COUNT, size=EMB_DIM, window=WINDOW_LEN, min_count=WORD_COUNT_MIN, workers=4, compute_loss=True, callbacks=[callback()] ) print(model) . Loss after epoch 0: 144.2177734375 Loss after epoch 1: 313.227783203125 Loss after epoch 2: 461.83636474609375 Loss after epoch 3: 563.6613159179688 Loss after epoch 4: 715.7371215820312 Word2Vec(vocab=68, size=64, alpha=0.025) . Why does the &quot;loss&quot; seem to increase after every epoch ?! Well - gensim devs decided to compute the cumulative loss across epochs. If you want loss value specific to each epoch - just output a diff() of this series. . Checkpoint - trained word2vec model . import gensim.models # Save model.wv.save_word2vec_format(&#39;data/w2v.model&#39;) # Load model = gensim.models.KeyedVectors.load_word2vec_format(&quot;data/w2v.model&quot;, binary=False) . Featurise each HN item . import numpy as np # Utility function that vectorises a single document def d2v(idx, doc): return np.hstack(([idx], np.mean(model[doc], axis=0))) . import dask # Use a dask wrapper to parallelise the task of vectorising all documents d_d2v = dask.delayed(d2v) n = len(docs_tokenized)//10 # For dev - work on 1/10th of total number of docs # n = len(docs_tokenized)//1 # Actual training # We will break-up the total vectorisation work into chunks so that we don&#39;t run out of memory chunks = list(range(0, n, n//10)) + [n] for k, _ in enumerate(chunks[:-1]): print(&#39;Preparing lazy tasks...&#39;) chunk_start = chunks[k] chunk_end = chunks[k+1] print([chunk_start, chunk_end]) tasks = ( d_d2v(idx, doc) for idx, doc in enumerate(docs_tokenized[chunk_start:chunk_end]) ) print(&#39;Computing tasks...&#39;) results = dask.compute(*tasks) with open(&#39;data/vecs_{:08d}_{:08d}&#39;.format(chunk_start, chunk_end), &#39;wb&#39;) as outpf: pickle.dump(results, outpf) print(outpf) print(&#39;Done.&#39;) . This will create bunch of pickle files - one for each chunk. Now, let&#39;s load each pickle and combine them all into one single numpy ndarray. . vecs = None for k, _ in enumerate(chunks[:-1]): chunk_start = chunks[k] chunk_end = chunks[k+1] print([chunk_start, chunk_end]) with open(&#39;data/vecs_{:08d}_{:08d}&#39;.format(chunk_start, chunk_end), &#39;rb&#39;) as inpf: vecs = np.vstack([vecs, *pickle.load(inpf)]) if type(vecs) == np.ndarray else np.vstack(pickle.load(inpf)) print(vecs.shape) # Save the vectors with open(&#39;data/vecs.pickle&#39;, &#39;wb&#39;) as outpf: pickle.dump(vecs, outpf) # Create and save the doc index idx_to_doc = { idx: doc for idx, doc in enumerate(docs_tokenized) } with open(&#39;data/idx_to_doc.pickle&#39;, &#39;wb&#39;) as outpf: pickle.dump(idx_to_doc, outpf) . Load trained vectors and doc index . Here I will simply load my &quot;pre-trained&quot; vectors and the document index. . import gensim.models # Load model model = gensim.models.KeyedVectors.load_word2vec_format(&quot;data/w2v.model&quot;, binary=False) # Load vectors with open(&#39;data/vecs.pickle&#39;, &#39;rb&#39;) as inpf: vecs = pickle.load(inpf) print(vecs.shape) # Load doc index with open(&#39;data/idx_to_doc.pickle&#39;, &#39;rb&#39;) as inpf: idx_to_doc = pickle.load(inpf) print(len(idx_to_doc), &#39;docs&#39;) . (3500863, 64) 3500863 docs . Let&#39;s explore the trained vectors a bit . import numpy as np import sklearn.metrics def get_nn_by_idx(idx_src, top_k=30): print(df_stories[&#39;title&#39;][idx_src]) sims = sklearn.metrics.pairwise.cosine_similarity( vecs[idx_src].reshape(1, -1), vecs ) sims = sims.reshape(-1) ranker = np.argsort(-sims) ranked_tpl = tuple(zip(df_stories[&#39;title&#39;][ranker[:top_k]], sims[ranker[:top_k]])) return ranked_tpl idx_src = 17300 get_nn_by_idx(idx_src) . Elon Musk: The mind behind Tesla, SpaceX, SolarCity . ((&#39;Elon Musk: The mind behind Tesla, SpaceX, SolarCity&#39;, 1.0000000000000002), (&#39;TED: Elon Musk: The mind behind Tesla, SpaceX, SolarCity&#39;, 0.9901475391454364), (&#39;Elon Musk at TED: The mind behind Tesla, SpaceX, SolarCity ...&#39;, 0.9762480544658949), (&#39;Elon Musk: The mind behind Tesla, SpaceX, SolarCity ... (TED interview video)&#39;, 0.9570489372431716), (&#39;Elon Musk / Tesla Harpooned Baseload Power&#39;, 0.9455763232082026), (&#34;Tesla Motors&#39; history, according to Elon Musk&#34;, 0.9409129143294029), (&#34;Watch Tesla and SpaceX CEO Elon Musk &#39;Wing It&#39;&#34;, 0.9301289785801119), (&#34;Tesla and SpaceX: Elon Musk&#39;s industrial empire [video]&#34;, 0.9295719274744014), (&#39;Elon Musk Left OpenAI to Focus on Tesla, SpaceX&#39;, 0.9259167457747793), (&#39;Tesla’s Elon Musk Hints at His Next Big Project: Electric Airplanes&#39;, 0.9253731125946242), (&#39;Elon Musk reveals SpaceX spacesuit&#39;, 0.9231230178852836), (&#34;Tesla: Elon Musk reveals latest &#39;masterplan&#39;&#34;, 0.9206816496333221), (&#39;Elon Musk: Predictions for Tesla&#39;, 0.9205715712645921), (&#39;Elon Musk on Running Tesla Motors and SpaceX&#39;, 0.9202363973460967), (&#39;Tesla boss Elon Musk hints at technology giveaway&#39;, 0.9199150245768559), (&#39;Tesla boss Elon Musk hints at technology giveaway&#39;, 0.9199150245768559), (&#39;The Secret Tesla Motors Master Plan – Elon Musk (2006)&#39;, 0.9190683096367459), (&#34;Elon Musk&#39;s Tesla Roadster&#34;, 0.9188396776229492), (&#39;Elon Musk Talks about Tesla, SpaceX and the Ad Astra School&#39;, 0.9185911170162555), (&#39;Elon Musk: Founder of Paypal, SpaceX, and Tesla Motors &#39;, 0.9179275121376558), (&#39;Elon Musk and the Hyperloop &#39;, 0.917497471287488), (&#39;Elon Musk New Bio: Tesla, SpaceX, Tunnels (yes, Tunnels) and OpenAI&#39;, 0.9170782079112136), (&#39;Elon Musk Debuts the Tesla Powerwall&#39;, 0.9165688065651327), (&#39;Elon Musk Debuts the Tesla Powerwall&#39;, 0.9165688065651327), (&#39;Tesla Founder Elon Musk Dreams Of Electric Airplanes&#39;, 0.9158676752681787), (&#39;Billionaire Elon Musk and the Hyperloop&#39;, 0.9157258670984152), (&#39;Elon Musk’s hyperloop&#39;, 0.9155317468252266), (&#39;Elon Musk before Tesla (1999)&#39;, 0.9137209099794992), (&#39;Elon Musk just unveiled the SpaceX spacesuit&#39;, 0.9136538223599378), (&#39;Elon Musk debunks electric space rockets after Simpsons lampooning&#39;, 0.9134915218911093)) . So our input doc (@idx 17300) had the title &quot;Elon Musk: The mind behind Tesla, SpaceX, SolarCity&quot;. The nearest neighbor docs that our model returned - are also about Elon Musk. OK. Looks like we are getting sensible output. Let&#39;s do more try-outs ! . get_nn_by_idx(42, top_k=5) . SEO ΕΝΤΟΣ ΣΕΛΙΔΑΣ – SEO – WEB DESIGN . ((&#39;Αποτυχημένα Πειράματα SEO – SEO | WEB DESIGN&#39;, 1.0000000000000002), (&#39;Η+καμπύλη+μάθησης+στο+SEO+-+SEO+|+WEB+DESIGN&#39;, 1.0000000000000002), (&#39;SEO ΕΝΤΟΣ ΣΕΛΙΔΑΣ – SEO – WEB DESIGN&#39;, 1.0000000000000002), (&#39;SEO+ΕΝΤΟΣ+ΣΕΛΙΔΑΣ+-+SEO+|+WEB+DESIGN&#39;, 1.0000000000000002), (&#39;Πόσο σημαντικό είναι το SEO στην κατασκευή ιστοσελίδων; – SEO – WEB DESIGN&#39;, 1.0000000000000002)) . get_nn_by_idx(1000, top_k=5) . What are the hidden cost of cloud computing? . ((&#39;What are the hidden cost of cloud computing?&#39;, 1.0000000000000004), (&#39;What are the Dangers of Cloud Computing?&#39;, 0.9697405649995038), (&#39;What are the new realities of the Cloud marketplace?&#39;, 0.948241031437423), (&#39;What are the main issues/shortcomings of cloud computing?&#39;, 0.9460991754426975), (&#39;What are the risks of cloud services?&#39;, 0.9428724582755382)) . get_nn_by_idx(5555, top_k=5) # Not so good :( . Limited Progress Seen Even as More Nations Step Up on Climate . ((&#39;Limited Progress Seen Even as More Nations Step Up on Climate&#39;, 1.0000000000000002), (&#39;EU on verge of overtaking US as the better place for entrepreneurs to reside&#39;, 0.8813214417831587), (&#39;The new coronavirus appears to take a greater toll on men than on women&#39;, 0.8766103721701922), (&#39;The Unexpected Step That Led Us to Transparent Salaries&#39;, 0.8754170062642075), (&#39;California wildfires sped up climate change as much as a whole year of power use&#39;, 0.8733405020767046)) . get_nn_by_idx(100001, top_k=5) . What to Expect in the New Microsoft Edge Insider Channels . ((&#39;What to Expect in the New Microsoft Edge Insider Channels&#39;, 1.0000000000000002), (&#39;Microsoft in 2013: what to expect&#39;, 0.939729428839874), (&#39;The Google Shift In Enterprise IT&#39;, 0.9307101135113099), (&#39;Upgrading to the New Microsoft Edge&#39;, 0.9286608938088898), (&#39;What’s new in Microsoft Edge&#39;, 0.9270582052153575)) . get_nn_by_idx(88888, top_k=5) # Not so good :( . The Technology Behind the U.S. Open . ((&#39;The Technology Behind the U.S. Open&#39;, 1.0), (&#39;The Alchemy of the U.S. Open Schedule &#39;, 0.9385653094871506), (&#39;The Innovation Behind the Technology&#39;, 0.9266781727797966), (&#39;The Future of the Open Internet&#39;, 0.9261051564185586), (&#39;The Open Office and the Spirit of Capitalism&#39;, 0.9246536131256814)) . Sometimes the &quot;nearest&quot; documents returned by our model are not so good. This could be largely due to imbalance in the representation across different topics or perhaps can be improved by tuning the model further. But overall, the nereast neighbor lookups are yielding sensible results. . def get_nn_by_text(text, top_k=30): print(text) doc = tokenizer.tokenize(clean_text(text)) sims = sklearn.metrics.pairwise.cosine_similarity( d2v(0, doc).reshape(1, -1)[:, 1:], vecs ) sims = sims.reshape(-1) ranker = np.argsort(-sims) ranked_tpl = tuple(zip(df_stories[&#39;title&#39;][ranker[:top_k]], sims[ranker[:top_k]])) return ranked_tpl text = &#39;Python dev tools&#39; get_nn_by_text(text) . Python dev tools . ((&#39;Java Dev Tools&#39;, 0.9672161434264985), (&#39;Tools for Python Developers&#39;, 0.9521089622128531), (&#39;Python debugging tools&#39;, 0.9447605911498455), (&#39;Developer Tools in Python&#39;, 0.9444010461622591), (&#39;Python tools&#39;, 0.9425476909117002), (&#39;12 PHP Debugging Tools for Developers&#39;, 0.9297497162130086), (&#39;Useful Tools for Python Developers&#39;, 0.9271513779246525), (&#39;Useful Tools for Python Developers&#39;, 0.9271513779246525), (&#39;Useful Tools for Python Developers&#39;, 0.9271513779246525), (&#39;Ruby development tools for Emacs and Vim&#39;, 0.9227354537698468), (&#39;JavaScript development tools – resources&#39;, 0.9213798799546349), (&#39;Javascript development tools - resources&#39;, 0.9213798799546349), (&#39;Top Visual Studio Code Extensions for Python Development&#39;, 0.9203817947689437), (&#34;Mozilla&#39;s Python tools&#34;, 0.9187298567082218), (&#39;Development environments for beginners: ruby, javascript, and python&#39;, 0.9140147448260572), (&#39;Advanced Git Tips for Python Developers&#39;, 0.9138558826895148), (&#39;Scala Tools&#39;, 0.9133397084093249), (&#39;Development environments for beginners: ruby, javascript, &amp; python&#39;, 0.9123347626644538), (&#39;VSCode Python remote development environments&#39;, 0.9122056690446374), (&#39;PHP Application Development-Some Tools&#39;, 0.9114191105552717), (&#39;Ansible Roles for Python Developers&#39;, 0.9111052151965933), (&#39;Modern PHP Development tools&#39;, 0.9110715581854112), (&#39;Python Testing Tools&#39;, 0.9106915532981802), (&#39;Chrome Dev Tools: JavaScript and Performance&#39;, 0.90987676408375), (&#39;Chrome Dev Tools: JavaScript and Performance&#39;, 0.90987676408375), (&#39;JavaScript Debugging Tips Part III – Advanced Google Chrome Developer Tools&#39;, 0.9097883856604116), (&#39;Resources for the Node.js Dev&#39;, 0.9089811994032215), (&#39;Python Tools for Visual Studio 2.0 Alpha: remote Linux debug, virtualenv, Django&#39;, 0.9088543317042411), (&#39;Python for Bash – DEV Community u200d u200d&#39;, 0.9088514719721184), (&#39;Python: IdleX - IDLE Extensions for Python to simplify code development&#39;, 0.9080812259757685)) . OK. That&#39;s not bad either! Congratulations - vectors are baked and ready for consumption. . import sklearn.manifold # Sample size n_limit = 10000 tsne = sklearn.manifold.TSNE( perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, metric=&#39;euclidean&#39;, verbose=1 ) X_tsne = tsne.fit_transform(vecs[:n_limit]) print(X_tsne.shape) . [t-SNE] Computing 91 nearest neighbors... [t-SNE] Indexed 10000 samples in 0.019s... [t-SNE] Computed neighbors for 10000 samples in 12.908s... [t-SNE] Computed conditional probabilities for sample 1000 / 10000 [t-SNE] Computed conditional probabilities for sample 2000 / 10000 [t-SNE] Computed conditional probabilities for sample 3000 / 10000 [t-SNE] Computed conditional probabilities for sample 4000 / 10000 [t-SNE] Computed conditional probabilities for sample 5000 / 10000 [t-SNE] Computed conditional probabilities for sample 6000 / 10000 [t-SNE] Computed conditional probabilities for sample 7000 / 10000 [t-SNE] Computed conditional probabilities for sample 8000 / 10000 [t-SNE] Computed conditional probabilities for sample 9000 / 10000 [t-SNE] Computed conditional probabilities for sample 10000 / 10000 [t-SNE] Mean sigma: 1.404193 [t-SNE] KL divergence after 250 iterations with early exaggeration: 94.332436 [t-SNE] KL divergence after 1000 iterations: 2.932338 (10000, 2) . Checkpoint - 2d tSNE vectors . # Save with open(&#39;data/X_tsne.pickle&#39;, &#39;wb&#39;) as otpf: pickle.dump(X_tsne, otpf) # Load with open(&#39;data/X_tsne.pickle&#39;, &#39;rb&#39;) as inpf: X_tsne = pickle.load(inpf) . import pickle # Load with open(&#39;data/X_tsne.pickle&#39;, &#39;rb&#39;) as inpf: X_tsne = pickle.load(inpf) . Visualising trained vectors via tSNE . Let&#39;s try to visualise our vectors. There are ~3M of them and that&#39;s too many for plotting. So we will sample some tSNE vectors and visualise them via a scatter plot. . from matplotlib import pyplot as plt fig, ax = plt.subplots() ax.scatter(X_tsne[:n_limit, 0], X_tsne[:n_limit, 1], s=1) plt.title(&#39;HN Story Titles&#39;) fig.set_size_inches(8, 8) . Hmmmm. There are some &quot;dense pockets&quot; but there is a lot of &quot;unstructured mass&quot; too. Let&#39;s explore these spaces a bit. . import plotly.graph_objects as go n_limit = 10000 fig = go.Figure( data=go.Scatter( x=X_tsne[:n_limit, 0], y=X_tsne[:n_limit, 1], text=df_stories[&#39;title&#39;][:n_limit].tolist(), mode=&#39;markers&#39;, marker=dict(size=3) ) ) fig.update_layout(title=&#39;HN Story Titles&#39;, height=800, width=800) fig.show() .",
            "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/hn/google/chrome/extension/unsupervised/ml/2020/05/24/HackerMark-featurise-titles.html",
            "relUrl": "/hn/google/chrome/extension/unsupervised/ml/2020/05/24/HackerMark-featurise-titles.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Part 1: Getting data from HackerNews",
            "content": "Where is the data ? . I had two options - either download the news data from HN using their API or get a (near) up-to-date dump via Kaggle. Yes! Kaggle is not just for getting into the world of competitive ML coding. It is a fabulous resource for a broad variety of structured datasets. Here is the link for - HN data. . . The header image says it has posts from 2006 to late 2017. But if you look inside the data tables you will find that it has data upto the current week. Current day and up to a couple previous days may not be in it. You will have to use the HN API to get that - but more on that later. For now all the HN data upto current minus two days will suffice. . Using Google&#39;s BigQuery to access the data source . We will write some SQL queries to explore the data, save it to the Kaggle&#39;s working directory. You get a (generous) 5GB of space in your working directory! That&#39;s where I will put HN data. Later, I will show you a nifty trick to download that data to your data processing machine. . First - fire up a Kaggle notebook. Start coding! . import pandas as pd from google.cloud import bigquery # Create a &quot;Client&quot; object client = bigquery.Client() . Our SQL client is ready. . Now lets&#39;s explore the HN dataset a bit. . query = &quot;&quot;&quot; SELECT COUNT(*) FROM bigquery-public-data.hacker_news.full WHERE type = &#39;story&#39; AND title IS NOT NULL; &quot;&quot;&quot; # Set up the query query_job = client.query(query) # API request - run the query, and return a pandas DataFrame query_job.to_dataframe() . Output: . f0_ . 0 | 3524454 | . So - there are about 3M rows of the type &#39;story&#39; that have a non-empty title. Why do we care about the title ? Well - we will be using the title field alone to build the news catalogue. . Let&#39;s see some rows. . query = &quot;&quot;&quot; SELECT * FROM bigquery-public-data.hacker_news.full WHERE type = &#39;story&#39; LIMIT 10; &quot;&quot;&quot; # Set up the query query_job = client.query(query) # API request - run the query, and return a pandas DataFrame query_job.to_dataframe() . . What is the lastest row in this table ? Lets find out! . query = &quot;&quot;&quot; SELECT MAX(timestamp) FROM bigquery-public-data.hacker_news.full WHERE type = &#39;story&#39; AND DATE(timestamp) &gt; &#39;2020-05-01&#39;; &quot;&quot;&quot; # Set up the query query_job = client.query(query) # API request - run the query, and return a pandas DataFrame query_job.to_dataframe() . I ran the query on 2020-05-20 9:30 p.m. IST. Here was the output: . f0_ . 0 | 2020-05-19 11:19:42+00:00 | . So - less than a day old. Not bad for a data source that is freely available and maintained by a 3rd party (not me, laziness !) . Fetch data . Let&#39;s fetch the data now. . query = &quot;&quot;&quot; SELECT title, id, timestamp FROM bigquery-public-data.hacker_news.full WHERE type = &#39;story&#39; AND title IS NOT NULL; &quot;&quot;&quot; # Set up the query query_job = client.query(query) # API request - run the query, and return a pandas DataFrame df_stories = query_job.to_dataframe() df_stories.shape . This query will run for a little longer but no longer than 10 minutes. Remember, we are getting all the HN stories that have a title and there are about 3M of them at time of this writing. Patience ! . Here&#39;s the output: (3524454, 3) . Naice (: We&#39;ve got &gt; 3.5 million stories to play with ! And for each of them we have - a HN id, a timestamp and a title. . Save data . Let&#39;s save our hard work locally i.e. inside the Kaggle working directory. . import pickle with open(&#39;hn.stories.dt.pickle&#39;, &#39;wb&#39;) as outf: pickle.dump(df_stories, outf) . Check our work: !ls -al --block-size=M *.pickle . Output: -rw-r--r-- 1 root root 250M May 20 16:10 hn.stories.dt.pickle . Thats good. Sweet 250 mega bytes of HN news data. . A nifty little trick . Here&#39;s a nifty trick that I had promised - to create a cute little download link! . from IPython.display import FileLink FileLink(r&#39;hn.stories.dt.pickle&#39;) . Output: hn.stories.dt.pickle . The output is a hyperlink to your pickle file. Use wget or your favorite download manager to get that file onto your data processing machine. I have used an AWS EC2 instance for further work. So lets say bye to Kaggle and hello to EC2. See you there - in the next blog post ! .",
            "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/hn/google/chrome/extension/unsupervised/ml/2020/05/20/HackerMark-get-data.html",
            "relUrl": "/hn/google/chrome/extension/unsupervised/ml/2020/05/20/HackerMark-get-data.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "HackerMark - a Google Chrome extension for HackerNews",
            "content": "What is HN ? . HackerNews, or more popularly referred to by its initials as simply HN, is this amazing NEWS resource hosted at https://news.ycombinator.com/. Users (&quot;hackers&quot;) submit NEWS items on topics interesting to themselves and other users. Items can be voted up/down and commented upon by member users. Its like reddit but without the group-by-topic organisation that is encouraged by sub-reddits. . Introducing HackerMark . A Google Chrome Extension that annotates each HN item with a tag. A screenshot: A [tag] is displayed under news item. In future versions, each tag will get a distinct color. . Components of the build . HN data | Unsupervised machine learning model | Ontology of tags | Tagger function | Backend and its API | Deploying to AWS | Google Chrome extension | . This will be a multi-part series of posts in which I will explain in detail - how I built and shipped HackerMark. Here is part-1. .",
            "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/hn/google/chrome/extension/unsupervised/ml/2020/05/19/Google-Chome-extension-for-HackerNews.html",
            "relUrl": "/hn/google/chrome/extension/unsupervised/ml/2020/05/19/Google-Chome-extension-for-HackerNews.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Pushkar Paranjpe is a researcher in the Big Data and Data Sciences dept. of Star India Pvt. Ltd. He is interested in OSS, machine learning, 3d printing, electronics, robots and startups. This is a place to document his side projects. When not in a lockdown mode - he likes: cycling and swimming. He also enjoys reading, playing PC / PS4 games / chess / board games. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pushkarparanjpe.github.io/kidepaha_fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}